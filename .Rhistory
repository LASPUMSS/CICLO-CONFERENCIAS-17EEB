source("E:/ARTICULOS-LASP/CICLO-CONFERENCIAS-17EEB/requeriments.R", echo=TRUE)
install.packages("ggplot2", dependencies = T)
install.packages("dplyr", dependencies = T)
install.packages("data.table", dependencies = T)
install.packages("rsample", dependencies = T)
install.packages("caret", dependencies = T)
install.packages("rpart", dependencies = T)
install.packages("tibble", dependencies = T)
install.packages("MASS", dependencies = T)
install.packages("tibble", dependencies = T)
install.packages("dplyr", dependencies = T)
install.packages("rpart", dependencies = T)
install.packages("caret", dependencies = T)
install.packages("MASS", dependencies = T)
install.packages("rsample", dependencies = T)
install.packages("data.table", dependencies = T)
install.packages("data.table", dependencies = T)
install.packages("rsample", dependencies = T)
install.packages("rpart", dependencies = T)
install.packages("MASS", dependencies = T)
install.packages("caret", dependencies = T)
install.packages("caret", dependencies = T)
install.packages("MASS", dependencies = T)
install.packages("rpart", dependencies = T)
install.packages("rpart", dependencies = T)
# Limpiar workspace
rm(list = ls())
#-> Importar librerias
library(rsample)      # data splitting
library(gbm)          # basic implementation
library(xgboost)      # a faster implementation of gbm
library(caret)        # an aggregator package for performing many machine learning models
library(h2o)          # a java-based platform
library(pdp)          # model visualization
library(ggplot2)      # model visualization
library(lime)         # model visualization
library(AmesHousing)  # Base de datos para trabajar
#-> Creacion de muestras de entrenamiento (70%) y evaluacion (30%).
set.seed(123)
ames_split = initial_split(AmesHousing::make_ames(), prop = .7)
ames_train = training(ames_split)
ames_test  = testing(ames_split)
#-> Implementacion basica, usando gbm
set.seed(123)
#-> Entrenar el modelo
gbm.fit = gbm(
formula = Sale_Price ~ .,
distribution = "gaussian",
data = ames_train,
n.trees = 10000,       # numero de arboles
interaction.depth = 1, # numero de nodos
shrinkage = 0.001,     # learning rate, step size, shrinkage, equivalentes.
cv.folds = 5,          # se implementa por validacion cruzada.
n.cores = NULL,        # usa todo el procesador
verbose = FALSE
)
#-> se imprimen los resultados
print(gbm.fit)
#-> Se calcula el RMSE
sqrt(min(gbm.fit$cv.error))
#-> Se grafica la funcion de perdida para los n arboles
gbm.perf(gbm.fit, method = "cv")
# Limpiar workspace
rm(list = ls())
#-> Importar las librerias
library("neuralnet")
library(MASS)
#-> Para asegurar replicabilidad de los resultados
set.seed(123)
#-> Importar la base de datos
data = Boston
str(data) # obtener una vista de los datos
#-> En redes neuronales, se deben normalizar los datos, para mejorar ajuste
#--> datos sin unidades, en la misma escala.
max_data = apply(data, 2, max)
min_data = apply(data, 2, min)
#--> x_{scaled} = \frac{x-x_{min}}{x_{max}-x_{min}}
data_scaled = scale(data,center = min_data, scale = max_data - min_data)
#--> Se toma la muestra completa y se rompe en muestra de entrenamiento (70%) y
#--> de evaluacion (30%).
index = sample(1:nrow(data),round(0.70*nrow(data)))
train_data = as.data.frame(data_scaled[index,])
test_data = as.data.frame(data_scaled[-index,])
#--> Se implementa el procedimiento de estimacion de la ANN
#----> Se extraen los nombres de las variables
n = names(data)
#----> Se crear recursivamente la formula de la ANN a estimar dep ~ todas las exp
f = as.formula(paste("medv ~", paste(n[!n %in% "medv"], collapse = " + ")))
#----> Se estima la red neuronal con 10 nodos ocultos (hidden=10) y resultado lineal
net_data = neuralnet(f,data=train_data,hidden=10,linear.output=T)
#----> Se grafica la red resultante
plot(net_data)
#----> Con esta red es posible hacer prediccion directamente
predict_net_test = compute(net_data,test_data[,1:13])
#----> Se calcula el RMSE para este pronostico
predict_net_test_start = predict_net_test$net.result*(max(data$medv)-min(data$medv))+min(data$medv)
test_start = as.data.frame((test_data$medv)*(max(data$medv)-min(data$medv))+min(data$medv))
MSE.net_data = sum((test_start - predict_net_test_start)^2)/nrow(test_start)
#----> Para evaluar la capacidad predictiva de esta ANN, se contrasta contra una
#----> regresion lineal.
#----> Se estima el modelo
Regression_Model = lm(medv~., data=data)
summary(Regression_Model)
#----> Se extrae la muestra de evaluacion
test = data[-index,]
#----> Se predice sobre la muestra de evaluacion
predict_lm = predict(Regression_Model,test)
#----> Se calcula el RMSE
MSE.lm = sum((predict_lm - test$medv)^2)/nrow(test)
#----> Se contrastan los dos RMSE
MSE.net_data
MSE.lm
#--> Notese que la red neural tiene un menor RMSE que la regresion lineal multiple.
# Limpiar workspace
rm(list = ls())
#-> Importar librerias requeridas
library(rsample)      # Dividir muestras
library(rpart)        # Implementacion basica de la metodologia
library(rpart.plot)   # Graficos de regression trees
library(caret)        # Un paquete que permite implementar varias tecnicas de ML
library(ipred)        # Bagging
library(AmesHousing)  # Base de datos para trabajar
library(ggplot2)
library(tidyr)
library(dplyr)
#-> Creacion de muestras de entrenamiento (70%) y evaluacion (30%).
set.seed(123)
ames_split = initial_split(AmesHousing::make_ames(), prop = .7)
ames_train = training(ames_split)
ames_test  = testing(ames_split)
#-> Implementacion basica
#-> Para hacer regression trees es necesario usar "anova" en la opcion "method"
m1 <- rpart(
formula = Sale_Price ~ .,
data    = ames_train,
method  = "anova"
)
m1
#-> Es posible ver esta misma informacion, pero visualmente
rpart.plot(m1)
#-> Si se desea ver como se disminuye el error a medida que el arbol va creciendo
#-> se puede usar la siguiente instruccion
plotcp(m1)
#-> Noten como el error no mejora ante cada nuevo nodo.
#-> Si se quiere evaluar la totalidad del arbol, sin podar, y ver como se comporta
#-> este indicador, se debe hacer lo siguiente:
#-> CLAVE: cp=0 desactiva el podado del arbol.
m2 = rpart(
formula = Sale_Price ~ .,
data    = ames_train,
method  = "anova",
control = list(cp = 0, xval = 10)
)
plotcp(m2)
abline(v = 12, lty = "dashed")
m1$cptable
#-> Igual, se puede tratar de refinar el arbol, mediante la calibracion de los
#-> hiperparametros del modelo: minsplit es el numero minimo de datos en cada nodo
#-> maxdepth es el numero maximo de nodos internos.
#-> Por ejemplo, se va a crecer un arbol con 10 obs por nodo y maximo 12 nodos internos
m3 <- rpart(
formula = Sale_Price ~ .,
data    = ames_train,
method  = "anova",
control = list(minsplit = 10, maxdepth = 12, xval = 10)
)
m3$cptable
#-> Por supuesto, este procedimiento puede hacerse automaticamente, sin tener que
#-> repetir el codigo para cada combinacion de hiperparametros.
hyper_grid <- expand.grid(
minsplit = seq(5, 20, 1),
maxdepth = seq(8, 15, 1)
)
head(hyper_grid)
#--> numero total de combinaciones
nrow(hyper_grid)
#-> Se aplican los experimentos
models = list()
for (i in 1:nrow(hyper_grid)) {
# get minsplit, maxdepth values at row i
minsplit = hyper_grid$minsplit[i]
maxdepth = hyper_grid$maxdepth[i]
# train a model and store in the list
models[[i]] = rpart(
formula = Sale_Price ~ .,
data    = ames_train,
method  = "anova",
control = list(minsplit = minsplit, maxdepth = maxdepth)
)
}
#-> Con esta funcion se puede hallar la combinacion optima de hiperparametros
#-> de todos los experimentos hechos.
#--> Se halla el minimo numero de nodos
get_cp = function(x) {
min    = which.min(x$cptable[, "xerror"])
cp = x$cptable[min, "CP"]
}
#--> Se halla el minimo error
get_min_error = function(x) {
min    = which.min(x$cptable[, "xerror"])
xerror = x$cptable[min, "xerror"]
}
hyper_grid %>%
mutate(
cp    = purrr::map_dbl(models, get_cp),
error = purrr::map_dbl(models, get_min_error)
) %>%
arrange(error) %>%
top_n(-5, wt = error)
optimal_tree = rpart(
formula = Sale_Price ~ .,
data    = ames_train,
method  = "anova",
control = list(minsplit = 6, maxdepth = 10, cp = 0.01)
)
#-> Hagamos prediccion con este modelo
pred = predict(optimal_tree, newdata = ames_test)
#-> Calculo del RMSE
RMSE_rpart = RMSE(pred = pred, obs = ames_test$Sale_Price)
#-> Grafico observado vs pronosticado
#--> Grafico de comparacion
plot(pred, type = "l", lty=1)
lines(ames_test$Sale_Price, type = "l", lty=1, col=3)
#-------------------------------------------------------------------------------
# Bagging
# Recuerden que este procedimiento esta entre Regression Tree y Random Forest
#-------------------------------------------------------------------------------
#-> Semilla para hacer reproducible el ejercicio
set.seed(123)
# Entrenar el modelo
bagged_m1 = bagging(
formula = Sale_Price ~ .,
data    = ames_train,
coob    = TRUE
)
bagged_m1
#-> En este acercamiento, mientras mas arboles se tenga, mejor. Sin embargo, se puede
#-> hallar un valor optimo de estos
#--> Evaluaremos una senda de entre 10 y 50 arboles
ntree = 10:50
#--> espacio para guardar los valores de OOB RMSE
rmse = vector(mode = "numeric", length = length(ntree))
#--> Implementando el experimento
for (i in seq_along(ntree)) {
set.seed(123)
#---> Implementar el modelo
model = bagging(
formula = Sale_Price ~ .,
data    = ames_train,
coob    = TRUE,
nbagg   = ntree[i]
)
#---> obtener el OOB error
rmse[i] = model$err
}
#--> Graficamos el resultado
plot(ntree, rmse, type = 'l', lwd = 2)
abline(v = 25, col = "red", lty = "dashed")
#-> Se especifica el espacio para hacer validacion cruzada
#-> 10-fold significa partir la muestra en 10 partes, 9 de entrenamiento, una de
#-> evaluacion y hacer 10 veces cada iteracion y guardar los residuales
ctrl = trainControl(method = "cv",  number = 10)
#-> implementar el modelo
bagged_cv = train(
Sale_Price ~ .,
data = ames_train,
method = "treebag",
trControl = ctrl,
importance = TRUE
)
#-> Revisar los resultados
bagged_cv
#-> Podemos graficar cuales son las variables mas importantes
plot(varImp(bagged_cv), 20)
#-> Podemos implementar prediccion y compararlo con el metodo anterior
pred_cv = predict(bagged_cv, ames_test)
#--> Grafico de comparacion
plot(pred, type = "l", lty=1)
lines(pred_cv, type = "l", lty=1, col=4)
lines(ames_test$Sale_Price, type = "l", lty=1, col=3)
#--> RMSE para este arbol
RMSE_cv = RMSE(pred_cv, ames_test$Sale_Price)
RMSE_rpart
RMSE_cv
# Limpiar workspace
rm(list = ls())
# Cargar librerias necesarias para estimacion
set.seed(123)
library(glmnet)  # clave para implementar Ridge
library(dplyr)
library(psych)
library(data.table)
library(ggplot2)
library(ggfortify)
library(caret)
#-> Se alistan las variables explicativas
setwd("D:/Dropbox/BR/Depto_Macro/Cursos/Banco Central de Bolivia/Códigos/Proyecciones lineales de alta dimensionalidad")
gdp.hf <- fread('us_gdp_monthly_ch12_sec6.csv',
select = c('date', 'coinc_indicators_index', 'employees', 'ind_prod', 'pers_inc', 'sales'))
